{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4yGJnSzMioTFiI6JUViui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasminBougammoura/nlp/blob/main/S_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "AwMTg_dLXD0v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z4-DVv1aSMLh"
      },
      "outputs": [],
      "source": [
        "with open('input.txt','r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "#print(''.join(chars))\n",
        "#print(vocab_size)"
      ],
      "metadata": {
        "id": "CRXh_spZTI1T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZE INPUT TEXT ---> see openai tokenizer\n",
        "# Creating a mapping from characters to integers\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)} #creating table for encoding\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "9AXGIhPoTqNa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE OUR DATA (INPUT TEXT) AND STORE IT INTO A TENSOR\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "#print(data.shape, data.dtype)\n",
        "#print(data[:1000])"
      ],
      "metadata": {
        "id": "t23WjpEUWQd5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING AND TEST DATASET SPLITTING\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "4ER26FioXZ3N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLING CHUNKS TO TRAIN\n",
        "\n",
        "block_size = 8 # what is the maximum context length for prediction?\n",
        "#train_data[:block_size+1]"
      ],
      "metadata": {
        "id": "2T2gxFVtXxmT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Understanding the code\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Xk9wal3jY6jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCHES SIZE (model parallelism)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "\n",
        "\n",
        "def get_batch(split): #function for any arbitrary split\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)- block_size, (batch_size,)) # 4 numbers randomly generated between 0 and (my data - block size)\n",
        "\n",
        "  # We positioned the batch and now we take the 1-dim tensors and stack'em up at rows\n",
        "  # so they all become a row in a 4x8 tensor\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y is the offset of x (so we add 1)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target is: {target}\")"
      ],
      "metadata": {
        "id": "qqpd08ojZaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING BIGRAM NEURAL NETWORK MODEL\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# I'm taking the index idx of my input, passing it into the embedding table and\n",
        "# what happens is that every integer of my input is referring to the table\n",
        "# and is going to pluck out a row of the table corresponding to its index\n",
        "# Torch is arranging all of this in a (Batch, Time, Channel) tensor - (4,8,65)\n",
        "# This is interpreted as the logits - scores for the next char sequence\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    logits = self.token_embedding_table(idx) # Arranged as (B = 4,T = 8,C = 65) tensor\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets=targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #gets predictions\n",
        "      logits, loss = self(idx)\n",
        "      #focus only on last time step\n",
        "      logits = logits[:,-1,:]\n",
        "      #apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      #sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      #append sampled index to the running sequence\n",
        "      idx = torch.cat((idx,idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "# We expect the loss to be the negative of loglikelihood, thus\n",
        "# -ln(1/65) = 4.174..\n",
        "'''\n",
        "idx = torch.zeros((1,1),dtype=torch.long) # 1 by 1 tensor holding a zero\n",
        "'''\n",
        "print(decode(m.generate(torch.zeros((1,1),dtype=torch.long),max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V62bwQzqgHAR",
        "outputId": "0ef63c4e-f293-4860-a3e2-13114578d0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL\n",
        "\n",
        "#Optimizer object\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  #sample a batch of data\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True) #zeroing gradients from previous steps\n",
        "  loss.backward() #obtaining gradients from current step\n",
        "  optimizer.step() #optimizing and updating parameters\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWXIZZxM9V6m",
        "outputId": "b4c482a9-04de-4677-c9d5-afae41ece7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1),dtype=torch.long),max_new_tokens = 400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIir_7U_F4cl",
        "outputId": "10235d9c-3fc8-42b8-b624-090eaef2ea63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZAnde.\n",
            "athave l.\n",
            "KEONH:\n",
            "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# self-attention trick\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_GhOAu0LUHO",
        "outputId": "59eab8e2-e18f-4fd6-8062-f461361d2015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t}x[b,t]\n",
        "xbow = torch.zeros((B,T,C)) # x bag of words\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # previous token included thus dimension is (t,C)\n",
        "    xbow[b,t] = torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "ytic5zVOMzua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can have the same computation but faster using matrix multiplication\n",
        "\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei/wei.sum(1,keepdim=True)\n",
        "xbow2 = wei @ x # (B,T,T,) @ (B,T,C) ---> (B,T,C)\n",
        "torch.allclose(xbow,xbow2) # due to numerical precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2izyfaKfN7bO",
        "outputId": "bc6a3222-89ef-45e4-cc04-62414296b9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can again use softmax to compute this calculation\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = 1)\n",
        "xbow3 = wei @ x"
      ],
      "metadata": {
        "id": "WEbSW0uzTy2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Small self-attention for a single individual head\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x) #(B,T,16)\n",
        "q = query(x) #(B,T,16)\n",
        "\n",
        "# Communication\n",
        "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = 1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ x\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "7f_fUtwQUP2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T, head_size)\n",
        "wei = q @ k.transpose(--2,-1) * head_size**-0.5 # for variance"
      ],
      "metadata": {
        "id": "Xwwl6V-EsEdg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}